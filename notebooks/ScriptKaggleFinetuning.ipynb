{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645c7c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:55:20.242811Z",
     "start_time": "2023-03-07T15:55:20.234101Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1d1f0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:55:42.497503Z",
     "start_time": "2023-03-07T15:55:20.503767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 132628, done.\u001b[K\n",
      "remote: Counting objects: 100% (242/242), done.\u001b[K\n",
      "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
      "remote: Total 132628 (delta 107), reused 159 (delta 69), pack-reused 132386\u001b[K\n",
      "Receiving objects: 100% (132628/132628), 127.68 MiB | 17.35 MiB/s, done.\n",
      "Resolving deltas: 100% (100037/100037), done.\n",
      "Processing ./transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from transformers==4.27.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from transformers==4.27.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from transformers==4.27.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from transformers==4.27.0.dev0) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from transformers==4.27.0.dev0) (2022.9.13)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from transformers==4.27.0.dev0) (21.3)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.27.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests->transformers==4.27.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests->transformers==4.27.0.dev0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests->transformers==4.27.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cedlop/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests->transformers==4.27.0.dev0) (1.26.12)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6672594 sha256=76ecf35936c94f2289fea1168b46bb67c55ddd8246967be608153cb1e83adc64\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wrpga2nh/wheels/70/ea/58/67f7d4f4fec71068ce76b61000e5d4b1f59aa771c8a09c24f8\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.9.0 huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.27.0.dev0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install transformers/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW, get_linear_schedule_with_warmup\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/optimization.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Iterable, Optional, Tuple, Union\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "#from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=88888)\n",
    "parser.add_argument(\"--model_name\", default=\"gpt2\", type=str)\n",
    "parser.add_argument(\"--max_seq_length\", default=512, type=int)\n",
    "parser.add_argument(\"--train_batch_size\", default=4, type=int)\n",
    "parser.add_argument(\"--valid_batch_size\", default=4, type=int)\n",
    "parser.add_argument(\"--num_train_epochs\", default=1, type=int)\n",
    "parser.add_argument(\"--warmup\", default=0.1, type=float)\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float)\n",
    "parser.add_argument(\"--input_text_path\", default='.', type=str)\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b97f7a",
   "metadata": {},
   "source": [
    "\n",
    "2. Prepare the data\n",
    "2.1 Combine the prompt and story, do a little text clean\n",
    "\n",
    "Download the text file from the above link. There are train, valid and test dataset in the original dataset. And the prompts and stories are in seperate files. For a example, the valid.wp_source has the writing promots and valid.wp_target has the corresponding stories. The train dataset is very large. Since kaggle notebook limits the kernel running time to 3 hours. I decide to take the valid dataset as my train dataset, and the test dataset as valid dataset.\n",
    "\n",
    "In order to feed the prompt an story together to GPT-2, I combine the prompts and stories togeter.Thus every line in the combined file includes the prompt and it's corresponding story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034af586",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH=args.input_text_path\n",
    "def combinetext(prompt, story):\n",
    "    fp=open(os.path.join(DATAPATH,prompt),encoding='utf8')\n",
    "    fs=open(os.path.join(DATAPATH,story),encoding='utf8')\n",
    "    prompts=fp.readlines()\n",
    "    stories=fs.readlines()\n",
    "    assert len(prompts)==len(stories)\n",
    "    combine=[]\n",
    "    for i in range(len(prompts)):\n",
    "        combine.append(prompts[i].rstrip()+' <sep> '+\" \".join(stories[i].split()[:300]))\n",
    "    return combine\n",
    "\n",
    "#do a littel text clean with punctuations\n",
    "def cleanpunctuation(s):\n",
    "    for p in '!,.:;?':\n",
    "        s=s.replace(' '+p,p)\n",
    "    s=s.replace(' '+'n\\'t','n\\'t')\n",
    "    s=s.replace(' '+'\\'s','\\'s')\n",
    "    s=s.replace(' '+'\\'re','\\'re')\n",
    "    s=s.replace(' '+'\\'ve','\\'ve')\n",
    "    s=s.replace(' '+'\\'ll','\\'ll')\n",
    "    s=s.replace(' '+'\\'am','\\'am')\n",
    "    s=s.replace(' '+'\\'m','\\'m')\n",
    "    s=s.replace(' '+'\\' m','\\'m')\n",
    "    s=s.replace(' '+'\\'m','\\'m')\n",
    "    s=s.replace(' '+'\\' ve','\\'ve')\n",
    "    s=s.replace(' '+'\\' s','\\'s')\n",
    "    s=s.replace('<newline>','\\n')\n",
    "    return s   \n",
    "\n",
    "train_text=combinetext('valid.wp_source', 'valid.wp_target')\n",
    "train_text=list(map(cleanpunctuation,train_text))\n",
    "valid_text=combinetext('test.wp_source', 'test.wp_target')\n",
    "valid_text=list(map(cleanpunctuation,valid_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_text[6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f4c1b",
   "metadata": {},
   "source": [
    "\n",
    "2.2 Tokenize and load to dataloader\n",
    "\n",
    "GPT-2 uses BPE to tokenize the text squence.BPE merges frequently co-occurred byte pairs in a greedy manner. In order to let the sequences in the same batch have the same length, I set the max length of sequence as 512, and truncate the longer sequence and pad the shorter sequence. Since the tokenizer function only return the input_ids and attention_mask. For training purpose, I need to feed the labels(targets) to the model. So I create labels sequence for every input_ids squence. In the label sequence,I rule out the padding tokens by set it to -100 to avoid compute loss on them. And also GPT-2 will automatically shift the labels to the right to match the inputs_ids, so I don't need to deal with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "inputs_train = tokenizer(train_text, padding=True,truncation=True,max_length=args.max_seq_length)\n",
    "inputs_valid=tokenizer(valid_text, padding=True,truncation=True,max_length=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e22212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(inputs):\n",
    "    labels=[]\n",
    "    for ids,attention_mask in zip(inputs['input_ids'],inputs['attention_mask']):\n",
    "        label=ids.copy()\n",
    "        real_len=sum(attention_mask)\n",
    "        padding_len=len(attention_mask)-sum(attention_mask)\n",
    "        label[:]=label[:real_len]+[-100]*padding_len\n",
    "        labels.append(label)\n",
    "    inputs['labels']=labels\n",
    "    \n",
    "create_labels(inputs_train)\n",
    "create_labels(inputs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_train['input_ids'][6])\n",
    "print(inputs_train['attention_mask'][6])\n",
    "print(inputs_train['labels'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset:\n",
    "    def __init__(self, inputs):\n",
    "        self.ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels=inputs['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return [torch.tensor(self.ids[item], dtype=torch.long),\n",
    "                torch.tensor(self.attention_mask[item], dtype=torch.long),\n",
    "                torch.tensor(self.labels[item], dtype=torch.long)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618aac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=args.train_batch_size\n",
    "valid_batch_size=args.valid_batch_size\n",
    "traindata=StoryDataset(inputs_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    traindata,\n",
    "    shuffle=False,\n",
    "    batch_size=train_batch_size)\n",
    "\n",
    "validdata=StoryDataset(inputs_valid)\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    validdata,\n",
    "    shuffle=False,\n",
    "    batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2c2f4",
   "metadata": {},
   "source": [
    "\n",
    "3. Model and optimizer\n",
    "3.1 Zero-shot story generate\n",
    "\n",
    "With the amazing transfomers pacakge, we can easily download the pretrained GPT-2 model. Before fine-tuning the model, I evaluate the model with valid dataset, and the average perplexity of evaluate results is 39. Let's see what is the score of perplexity after fine-tuning later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3475db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "model.eval()\n",
    "eval_loss=[]\n",
    "for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
    "    d1,d2,d3=inputs\n",
    "    d1=d1.to('cuda')        \n",
    "    d2=d2.to('cuda')\n",
    "    d3=d3.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "        batch_loss=output[0]\n",
    "    eval_loss+=[batch_loss.cpu().item()]\n",
    "    del batch_loss\n",
    "eval_loss=np.mean(eval_loss)\n",
    "perplexity=math.exp(eval_loss)\n",
    "print(f'The average perplexity for valid dataset before fine-tuning is {perplexity}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca64708",
   "metadata": {},
   "source": [
    "Let's pick a prompt from the valid dataset and input it into the model, have the model generate a 300 words long story. The output stories is really great! I use the generate method comes with the model. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling. The meanings of key arguments are below:\n",
    "1)do_sample: if set to False greedy decoding is used.\n",
    "2)The temperature is used to module the next token probabilities.\n",
    "3)top_k is the number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "4)top_p is the cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling.\n",
    "5)repetition_penalty is the parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=valid_text[300][:valid_text[300].find('<sep>')]\n",
    "target=valid_text[300][valid_text[300].find('<sep>')+5:]\n",
    "\n",
    "def generate_story(prompt,target,k=0,p=0.9,output_length=300,temperature=1,num_return_sequences=3,repetition_penalty=1.0):\n",
    "    print(\"====prompt====\\n\")\n",
    "    print(prompt+\"\\n\")\n",
    "    print('====target story is as below===\\n')\n",
    "    print(target+\"\\n\")\n",
    "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=encoded_prompt,\n",
    "        max_length=output_length,\n",
    "        temperature=temperature,\n",
    "        top_k=k,\n",
    "        top_p=p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences\n",
    "    )\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        # Remove all text after eos token\n",
    "        text = text[: text.find(tokenizer.eos_token)]\n",
    "        print(text)\n",
    "\n",
    "generate_story(prompt,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011091c",
   "metadata": {},
   "source": [
    "\n",
    "3.1 Fine-tune the model\n",
    "\n",
    "The number of training samples is 15620. With one GPU to train the model, it tooks about 21 minutes to run 1 epoch. After 1 epoche learning, the perplexity for valid dataset is about 24, which is better than the score before fine- tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = args.num_train_epochs\n",
    "training_steps_per_epoch=len(train_dataloader)\n",
    "total_num_training_steps = int(training_steps_per_epoch*num_train_epochs)\n",
    "weight_decay=0\n",
    "learning_rate=args.learning_rate\n",
    "adam_epsilon=1e-8\n",
    "warmup_steps=int(total_num_training_steps*args.warmup)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Total_num_training_step = {}\".format(total_num_training_steps))\n",
    "print(\"  Num Epochs = {}\".format(num_train_epochs))\n",
    "print(f\"  Train_batch_size per device = {train_batch_size}\")\n",
    "print(f\"  Valid_batch_size per device = {valid_batch_size}\")\n",
    "model.to('cuda')\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f\"Start epoch{epoch+1} of {num_train_epochs}\")\n",
    "    train_loss=0\n",
    "    epoch_iterator = tqdm(train_dataloader,desc='Iteration')\n",
    "    model.train()\n",
    "    model.zero_grad()    \n",
    "    for _, inputs in enumerate(epoch_iterator):        \n",
    "        d1,d2,d3=inputs\n",
    "        d1=d1.to('cuda')\n",
    "        d2=d2.to('cuda')\n",
    "        d3=d3.to('cuda')\n",
    "        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "        batch_loss=output[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        train_loss+=batch_loss.item()\n",
    "        epoch_iterator.set_description('(batch loss=%g)' % batch_loss.item())\n",
    "        del batch_loss\n",
    "    print(f'Average train loss per example={train_loss/training_steps_per_epoch} in epoch{epoch+1}')    \n",
    "    print(f'Starting evaluate after epoch {epoch+1}')\n",
    "    eval_loss=[]    \n",
    "    model.eval()    \n",
    "    for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n",
    "        d1,d2,d3=inputs\n",
    "        d1=d1.to('cuda')        \n",
    "        d2=d2.to('cuda')\n",
    "        d3=d3.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=d1, attention_mask=d2,labels=d3)\n",
    "            batch_loss=output[0]\n",
    "        eval_loss+=[batch_loss.cpu().item()]\n",
    "        del batch_loss\n",
    "    eval_loss=np.mean(eval_loss)\n",
    "    perplexity=math.exp(eval_loss)\n",
    "    print(f'Average valid loss per example={eval_loss} in epoch{epoch+1}')    \n",
    "    print(f'Perplextiy for valid dataset in epoch{epoch+1} is {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f14fcf",
   "metadata": {},
   "source": [
    "\n",
    "3.2 Generate stories\n",
    "\n",
    "I use the fine-tuened model to generate stories with the same prompt I used before fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt=valid_text[300][:valid_text[300].find('<sep>')]\n",
    "target=valid_text[300][valid_text[300].find('<sep>')+5:]\n",
    "generate_story(prompt,target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b855ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad10f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fb063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85254ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2bbe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487f4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
