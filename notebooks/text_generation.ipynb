{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.083826Z",
     "start_time": "2023-02-24T15:09:19.090778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 16:09:20.419885: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 16:09:20.539417: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:20.539431: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 16:09:20.555619: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-24 16:09:21.202330: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:21.202397: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:21.202402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# DATA MANIPULATION\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "# DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#Tenserflow\n",
    "import tensorflow.keras as tk\n",
    "\n",
    "from tensorflow.keras import Sequential, layers, regularizers\n",
    "\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Flatten\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# VIEWING OPTIONS IN THE NOTEBOOK\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation using a RNN ‚úçÔ∏è "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Our goal is to use a dataset of Shakespeare's writing from http://karpathy.github.io/2015/05/21/rnn-effectiveness/ in order to generate Shakespeare like texts from our own prompts! **Our model will take in 100 characters and predict the 101st character.** To predict an entire paragraph we can call our model over and over again using our generated characters (i.e character 2-100 + our generated 101 to predict 102)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "### 1.1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.121458Z",
     "start_time": "2023-02-24T15:09:22.086460Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:11.935727Z",
     "iopub.status.busy": "2022-12-14T13:32:11.935518Z",
     "iopub.status.idle": "2022-12-14T13:32:13.873227Z",
     "shell.execute_reply": "2022-12-14T13:32:13.872259Z"
    },
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "### 1.2) Get the data üìï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the helper function below üëá you can see it downloads us the data in the filename **shakespeare.txt** and returns us the file path to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.159539Z",
     "start_time": "2023-02-24T15:09:22.125807Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.877626Z",
     "iopub.status.busy": "2022-12-14T13:32:13.876904Z",
     "iopub.status.idle": "2022-12-14T13:32:13.931441Z",
     "shell.execute_reply": "2022-12-14T13:32:13.930866Z"
    },
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_data = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "### 1.3) Have a look at the data üîé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can open the file and read it as a string (we have to decode it to make a string rather than a byte string): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.201351Z",
     "start_time": "2023-02-24T15:09:22.162258Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.934985Z",
     "iopub.status.busy": "2022-12-14T13:32:13.934481Z",
     "iopub.status.idle": "2022-12-14T13:32:13.939701Z",
     "shell.execute_reply": "2022-12-14T13:32:13.939141Z"
    },
    "id": "aavnuByVymwK"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_data, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.240569Z",
     "start_time": "2023-02-24T15:09:22.204321Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.942844Z",
     "iopub.status.busy": "2022-12-14T13:32:13.942304Z",
     "iopub.status.idle": "2022-12-14T13:32:13.945817Z",
     "shell.execute_reply": "2022-12-14T13:32:13.945271Z"
    },
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 2Ô∏è‚É£ Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### 2.1) Vectorize the text\n",
    "\n",
    "Before training, you need to convert the strings to a numerical representation. \n",
    "\n",
    "The [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layer can convert each character into a numeric ID. This layer just needs the text to be split into tokens first. You can use the helper function [tf.strings.unicode_split](https://www.tensorflow.org/api_docs/python/tf/strings/unicode_split) to achieve that like the example below üëá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.347424Z",
     "start_time": "2023-02-24T15:09:22.243846Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.966911Z",
     "iopub.status.busy": "2022-12-14T13:32:13.966697Z",
     "iopub.status.idle": "2022-12-14T13:32:17.404330Z",
     "shell.execute_reply": "2022-12-14T13:32:17.403658Z"
    },
    "id": "a86OoYtO01go"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 16:09:22.304475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-24 16:09:22.304605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304875: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:09:22.304968: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-24 16:09:22.305640: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Generate the vocab üìñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Generate a list of **unique characters** in our text and save it in the variable **`vocab`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.402359Z",
     "start_time": "2023-02-24T15:09:22.349730Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s4f1q3iqY8f"
   },
   "source": [
    "‚ùì Now create the `tf.keras.layers.StringLookup` layer and save it as `ids_from_chars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.450678Z",
     "start_time": "2023-02-24T15:09:22.405063Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.408060Z",
     "iopub.status.busy": "2022-12-14T13:32:17.407548Z",
     "iopub.status.idle": "2022-12-14T13:32:17.423673Z",
     "shell.execute_reply": "2022-12-14T13:32:17.423064Z"
    },
    "id": "6GMlCe3qzaL9",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary markdown='span'>üí° If you get stuck</summary>\n",
    "\n",
    "```python\n",
    "tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX_jbgQqfOi"
   },
   "source": [
    "It converts from tokens to character IDs based on the vocab we passed to it. \n",
    "\n",
    "‚ùì Use the layer below üëá and edit `chars` variable above and see what happens when you add characters outside the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.496926Z",
     "start_time": "2023-02-24T15:09:22.455201Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.427404Z",
     "iopub.status.busy": "2022-12-14T13:32:17.426909Z",
     "iopub.status.idle": "2022-12-14T13:32:17.434684Z",
     "shell.execute_reply": "2022-12-14T13:32:17.434039Z"
    },
    "id": "WLv5Q_2TC2pc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "To generate text, it will also be important to **invert this representation** and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uenivzwqsDhp"
   },
   "source": [
    "‚ùóÔ∏è Here instead of passing the original vocabulary generated with `sorted(set(text))`, use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` to get the vocabulary assigned to the previous `ids_from_chars` layer. \n",
    "\n",
    "This way, we also have a `[UNK]` string for unknown characters outside our original representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.550052Z",
     "start_time": "2023-02-24T15:09:22.499439Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.438253Z",
     "iopub.status.busy": "2022-12-14T13:32:17.437740Z",
     "iopub.status.idle": "2022-12-14T13:32:17.449424Z",
     "shell.execute_reply": "2022-12-14T13:32:17.448738Z"
    },
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqTDDxS-s-H8"
   },
   "source": [
    "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.593528Z",
     "start_time": "2023-02-24T15:09:22.553106Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.452733Z",
     "iopub.status.busy": "2022-12-14T13:32:17.452253Z",
     "iopub.status.idle": "2022-12-14T13:32:17.457574Z",
     "shell.execute_reply": "2022-12-14T13:32:17.456992Z"
    },
    "id": "c2GCh0ySD44s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FeW5gqutT3o"
   },
   "source": [
    "‚úçÔ∏è We use `tf.strings.reduce_join` to join the characters back into strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.668188Z",
     "start_time": "2023-02-24T15:09:22.596540Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.461040Z",
     "iopub.status.busy": "2022-12-14T13:32:17.460453Z",
     "iopub.status.idle": "2022-12-14T13:32:17.506637Z",
     "shell.execute_reply": "2022-12-14T13:32:17.505960Z"
    },
    "id": "zxYI-PeltqKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define a function `text_from_ids` that takes a tensor of ids and returns the corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:22.710145Z",
     "start_time": "2023-02-24T15:09:22.671888Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.509989Z",
     "iopub.status.busy": "2022-12-14T13:32:17.509440Z",
     "iopub.status.idle": "2022-12-14T13:32:17.512781Z",
     "shell.execute_reply": "2022-12-14T13:32:17.512180Z"
    },
    "id": "w5apvBDn9Ind",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def text_from_ids(textes):\n",
    "    return tf.strings.reduce_join(chars_from_ids(textes), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the **test** below to check you are able to go back and forth from **text -> ids -> text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.204927Z",
     "start_time": "2023-02-24T15:09:22.712846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cedlop/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cedlop/code/CedrikGiau/data-text-generation/tests\n",
      "plugins: asyncio-0.19.0, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_helpers.py::TestHelpers::test_chars_to_ids \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 50%]\u001b[0m\n",
      "test_helpers.py::TestHelpers::test_ids_to_chars \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/helpers.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed helpers step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "test_texts = ['LeWagon', 'NLP`']\n",
    "test_chars = tf.strings.unicode_split(test_texts, input_encoding='UTF-8')\n",
    "test_ids = ids_from_chars(test_chars)\n",
    "test_reverse = text_from_ids(test_ids)\n",
    "result = ChallengeResult('helpers', ids=list(test_ids[0].numpy()), chars=test_reverse[1].numpy())\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) The dataset üöö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì First split our whole text using `unicode_split` and convert them all with `ids_from_chars`, to get all of our text as a single continuous array saved as `all_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.456854Z",
     "start_time": "2023-02-24T15:09:23.206914Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.516290Z",
     "iopub.status.busy": "2022-12-14T13:32:17.515731Z",
     "iopub.status.idle": "2022-12-14T13:32:17.862389Z",
     "shell.execute_reply": "2022-12-14T13:32:17.861710Z"
    },
    "id": "UopbsKi88tm5",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "all_ids=ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make a tensorflow dataset object with that array. This is an object which allows us to write pipelines to transform our data into the format needed for our model to read it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.494921Z",
     "start_time": "2023-02-24T15:09:23.459375Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.866209Z",
     "iopub.status.busy": "2022-12-14T13:32:17.865672Z",
     "iopub.status.idle": "2022-12-14T13:32:17.870573Z",
     "shell.execute_reply": "2022-12-14T13:32:17.869940Z"
    },
    "id": "qmxrYDCTy-eL"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method allows us to set how many characters we should take at a time! In our case we want **101**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.542567Z",
     "start_time": "2023-02-24T15:09:23.497596Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.909371Z",
     "iopub.status.busy": "2022-12-14T13:32:17.908760Z",
     "iopub.status.idle": "2022-12-14T13:32:17.926465Z",
     "shell.execute_reply": "2022-12-14T13:32:17.925781Z"
    },
    "id": "BpdjRO2CzOfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(101, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PHW902-4oZt"
   },
   "source": [
    "It's easier to see  if we join the tokens back into strings üëá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.591903Z",
     "start_time": "2023-02-24T15:09:23.545321Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.929732Z",
     "iopub.status.busy": "2022-12-14T13:32:17.929193Z",
     "iopub.status.idle": "2022-12-14T13:32:17.944697Z",
     "shell.execute_reply": "2022-12-14T13:32:17.943979Z"
    },
    "id": "QO32cMWu4a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For training you'll need a dataset of `(input, label)` pairs, where `input` and \n",
    "`label` are sequences. \n",
    "\n",
    "Even though we are predicting one character at a time, the sequence at each time step consists of the:\n",
    "\n",
    "1. `input` which is the `n` characters in the sequence up to the `n+1` character we want to predict\n",
    "2. `label` which is the predicted character and `n-1` characters leading up to it.\n",
    "\n",
    "For example if the text is `\"Hello\"`. The input sequence would be `\"Hell\"`, and the target sequence `\"ello\"`.\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary markdown='span'>ü§î Why do we have a target of <strong>ello</strong> if our goal is only to predict <strong>o</strong>? Click here for an explanation.</summary>\n",
    "\n",
    "It is much more stable to train a model this way. If **H** was only updated by the back propagation from the predict of **o** it would be very weakly updated. This problem would be even worse with 100 characters between!\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "‚ùì Write a function `split_input_target` which converts a sequence to a `(input, label)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.631824Z",
     "start_time": "2023-02-24T15:09:23.594453Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.947952Z",
     "iopub.status.busy": "2022-12-14T13:32:17.947412Z",
     "iopub.status.idle": "2022-12-14T13:32:17.950819Z",
     "shell.execute_reply": "2022-12-14T13:32:17.950262Z"
    },
    "id": "9NGu-FkO_kYU",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def split_input_target(seq):\n",
    "    pre_input = seq[:-1]\n",
    "    label= seq[1:]\n",
    "    return (pre_input, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we map the function to our dataset. This applies it to every element in the dataset, this is part of the reason `tensorflow` datasets are so powerful for preprocessing data! üôå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.710722Z",
     "start_time": "2023-02-24T15:09:23.634413Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.960675Z",
     "iopub.status.busy": "2022-12-14T13:32:17.960150Z",
     "iopub.status.idle": "2022-12-14T13:32:17.998126Z",
     "shell.execute_reply": "2022-12-14T13:32:17.997551Z"
    },
    "id": "B9iKPXkw5xwa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout what our **`dataset`** looks like now üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.770019Z",
     "start_time": "2023-02-24T15:09:23.713455Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.001470Z",
     "iopub.status.busy": "2022-12-14T13:32:18.000984Z",
     "iopub.status.idle": "2022-12-14T13:32:18.026512Z",
     "shell.execute_reply": "2022-12-14T13:32:18.025795Z"
    },
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 2.4) Optimizing the dataset üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow [datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) we define the batch size before we fit the model. We also:\n",
    "\n",
    "- shuffle the dataset\n",
    "- prefetch (this gets the next N elements ready) - super important when we are loading data from disk to have it ready for the next batch without wasting GPU time! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:23.817159Z",
     "start_time": "2023-02-24T15:09:23.772797Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.029784Z",
     "iopub.status.busy": "2022-12-14T13:32:18.029273Z",
     "iopub.status.idle": "2022-12-14T13:32:18.039620Z",
     "shell.execute_reply": "2022-12-14T13:32:18.039073Z"
    },
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the **test** below to check you have a working dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:09:24.158382Z",
     "start_time": "2023-02-24T15:09:23.820003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cedlop/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cedlop/code/CedrikGiau/data-text-generation/tests\n",
      "plugins: asyncio-0.19.0, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_dataset.py::TestDataset::test_input_shape \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 50%]\u001b[0m\n",
      "test_dataset.py::TestDataset::test_output_shape \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/dataset.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed dataset step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('dataset', input_shape=tuple(dataset.element_spec[0].shape), output_shape=tuple(dataset.element_spec[1].shape))\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 3Ô∏è‚É£ Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Define the model üîÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "This section defines the model as a [`keras.Model`](https://keras.io/api/models/model/) subclass which you won't have seen before (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
    "\n",
    "This model has three layers:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì The **model** is quite different than how we have defined models so far. Take a few minutes to try and understand the code. \n",
    "\n",
    "- The first section is the `__init__` here we define layers using `self.layer_name = layer`\n",
    "- The second section is where we define how to use the layers when we are given an input. You can see we call the layers similarly to the [Keras functional API](https://keras.io/guides/functional_api/) but we the flexibility to include `if` statements and other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:15:20.023182Z",
     "start_time": "2023-02-24T15:15:19.986002Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.049811Z",
     "iopub.status.busy": "2022-12-14T13:32:18.049307Z",
     "iopub.status.idle": "2022-12-14T13:32:18.054723Z",
     "shell.execute_reply": "2022-12-14T13:32:18.054087Z"
    },
    "id": "wj8HQ2w8z4iO"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, 256)\n",
    "        self.gru = tf.keras.layers.GRU(1024,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:15:20.358823Z",
     "start_time": "2023-02-24T15:15:20.310917Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.057787Z",
     "iopub.status.busy": "2022-12-14T13:32:18.057286Z",
     "iopub.status.idle": "2022-12-14T13:32:18.071762Z",
     "shell.execute_reply": "2022-12-14T13:32:18.071185Z"
    },
    "id": "IX58Xj9z47Aw"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size1 = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "model = MyModel(vocab_size=vocab_size1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "‚ùóÔ∏è For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "### 3.2) Check the model üî¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets call the untrained model of our first piece of data üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:15:24.109259Z",
     "start_time": "2023-02-24T15:15:21.837291Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.075434Z",
     "iopub.status.busy": "2022-12-14T13:32:18.074933Z",
     "iopub.status.idle": "2022-12-14T13:32:20.720603Z",
     "shell.execute_reply": "2022-12-14T13:32:20.719812Z"
    },
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model you need to sample from the output distributions. \n",
    "\n",
    "- This distribution is defined by the logits over the character vocabulary.\n",
    "- ‚ùó It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:15:25.753055Z",
     "start_time": "2023-02-24T15:15:25.714757Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.741781Z",
     "iopub.status.busy": "2022-12-14T13:32:20.741240Z",
     "iopub.status.idle": "2022-12-14T13:32:20.747723Z",
     "shell.execute_reply": "2022-12-14T13:32:20.747167Z"
    },
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:15:26.993924Z",
     "start_time": "2023-02-24T15:15:26.953855Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.751122Z",
     "iopub.status.busy": "2022-12-14T13:32:20.750595Z",
     "iopub.status.idle": "2022-12-14T13:32:20.754941Z",
     "shell.execute_reply": "2022-12-14T13:32:20.754313Z"
    },
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "### 3.3) Train the model üèãÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeOXriLcymww"
   },
   "source": [
    "‚ùì Compile the model with the **correct loss** and an optimizer\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "    <summary markdown='span'>üí° Click here for the loss if you're stuck</summary>\n",
    "\n",
    "    You should use the <code>tf.keras.losses.SparseCategoricalCrossentropy</code> loss.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:15:29.842040Z",
     "start_time": "2023-02-24T15:15:29.807149Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:49:38.267239Z",
     "start_time": "2023-02-24T15:49:38.237502Z"
    }
   },
   "outputs": [],
   "source": [
    "my_loss = SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:49:50.268299Z",
     "start_time": "2023-02-24T15:49:50.234139Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss =my_loss, optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the **test** below to check you have a good model before beginning to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:25:05.522327Z",
     "start_time": "2023-02-24T15:25:01.590861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/cedlop/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/cedlop/code/CedrikGiau/data-text-generation/tests\n",
      "plugins: asyncio-0.19.0, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_model.py::TestModel::test_loss \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 50%]\u001b[0m\n",
      "test_model.py::TestModel::test_output \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 3.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('model', loss=type(model.loss), output_weights=model.trainable_weights[5].shape[0])\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training within reasonable time, we will use **just 5 epochs** (you can increase this later if you like) to train the model. \n",
    "\n",
    "This will still take about 10 mins so grab a coffee ‚òïÔ∏è while you wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T16:08:44.887355Z",
     "start_time": "2023-02-24T15:50:32.897644Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.930017Z",
     "iopub.status.busy": "2022-12-14T13:32:20.929797Z",
     "iopub.status.idle": "2022-12-14T13:34:54.171315Z",
     "shell.execute_reply": "2022-12-14T13:34:54.170464Z"
    },
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 224s 1s/step - loss: 1.9979 - accuracy: 0.4175\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 218s 1s/step - loss: 1.6303 - accuracy: 0.5144\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 217s 1s/step - loss: 1.4694 - accuracy: 0.5561\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 217s 1s/step - loss: 1.3766 - accuracy: 0.5796\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 217s 1s/step - loss: 1.3110 - accuracy: 0.5962\n",
      "CPU times: user 3h 22min 42s, sys: 11min 2s, total: 3h 33min 44s\n",
      "Wall time: 18min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(dataset, epochs=5, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 4Ô∏è‚É£ Generate text üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 4.1) Generation model ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to edit our model for generation, the code below looks excessive so lets break it down:\n",
    "\n",
    "- We will inherit from Keras base model and pass our previously defined model to the `__init__` method\n",
    "- We will also create a mask which add a value of **negative infinity** for the unknown character **`[UNK]`** used to denote characters outside of our vocab as we never want our model to generate this character.\n",
    "- We **sample** and **squeeze** to get the predicted ids.\n",
    "- We pass the state back to allow us to feed it back into the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:43:07.474539Z",
     "start_time": "2023-02-24T15:43:07.439229Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:34:54.175589Z",
     "iopub.status.busy": "2022-12-14T13:34:54.175313Z",
     "iopub.status.idle": "2022-12-14T13:34:54.183593Z",
     "shell.execute_reply": "2022-12-14T13:34:54.182961Z"
    },
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T16:08:44.944869Z",
     "start_time": "2023-02-24T16:08:44.890355Z"
    }
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 4.2) Using the model üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9yDoa0G3IgQ"
   },
   "source": [
    "Now we can run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences but pretty impressive for the training time! üôå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Play around with the input text and the number of predict characters and see what your model creates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-24T15:58:15.908Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T13:34:54.205117Z",
     "iopub.status.busy": "2022-12-14T13:34:54.204676Z",
     "iopub.status.idle": "2022-12-14T13:34:56.855564Z",
     "shell.execute_reply": "2022-12-14T13:34:56.854833Z"
    },
    "id": "ST7PSyk9t1mT"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "states = None\n",
    "next_char = tf.constant(['Juliet: Where are you, Romeo?'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "üèÅ Even though the results could be improved significantly it is quite incredible what the model learnt in **only five** epochs! Next you can up the epochs or try using the model on some text of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-24T15:58:17.440Z"
    },
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:11.928547Z",
     "iopub.status.busy": "2022-12-14T13:32:11.927961Z",
     "iopub.status.idle": "2022-12-14T13:32:11.931971Z",
     "shell.execute_reply": "2022-12-14T13:32:11.931377Z"
    },
    "id": "GCCk8_dHpuNf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-24T15:58:17.710Z"
    }
   },
   "outputs": [],
   "source": [
    "!git add text_generation.ipynb\n",
    "\n",
    "!git commit -m 'Completed NPL Challenges'\n",
    "\n",
    "!git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
