{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:44.489848Z",
     "start_time": "2023-02-24T13:52:39.880419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 14:52:41.572145: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 14:52:41.985064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:41.985079: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 14:52:42.032338: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-24 14:52:43.012666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:43.012733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:43.012738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# DATA MANIPULATION\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "# DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#Tenserflow\n",
    "import tensorflow.keras as tk\n",
    "\n",
    "from tensorflow.keras import Sequential, layers, regularizers\n",
    "\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Flatten\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# VIEWING OPTIONS IN THE NOTEBOOK\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with CNN\n",
    "\n",
    "### Exercise objectives:\n",
    "\n",
    "- Use CNN instead of RNN for NLP\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "⚠️ **DISCLAIMER** ⚠️ **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:46.804670Z",
     "start_time": "2023-02-24T13:52:44.491839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 14:52:45.132449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-24 14:52:45.132538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132664: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132689: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132742: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-24 14:52:45.132747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-24 14:52:45.135168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "### Run this cell to load the data ###\n",
    "######################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that to do NLP, you need to go through one of the following options, as shown here: \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "But in both cases, you can replace the recurrent layer (top part) by a CNN layer. We will go into both, starting from the one on the left were the embedding is learned in the network.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Part 1 : Concatenate a Keras Embedding with a Conv1D🔥 \n",
    "\n",
    "Let's train a fancy network here. \n",
    "\n",
    "Each of your words is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). So, all your sentences are actually represented as matrices once embedded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate in the embedding space. As shown here\n",
    "\n",
    "<img src=\"image_comparison.png\" width=\"500px\" />\n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolution on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2 dimensional as the filters can move up and down, and left and right. In the case of our sentences, we want the kernel to move _only_ in the word by word direction (The alternative, moving coordinate by coordinate of the embedding space doesn't make much sense).\n",
    "\n",
    "So let's create a model that use convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the data\n",
    "\n",
    "❓ **Question** ❓ You will need to prepare your data. First, tokenize them. Then, you need to pad them (use a value `maxlen` equal to 150). You also might need to compute the size of your vocabulary ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:47.808011Z",
     "start_time": "2023-02-24T13:52:46.807425Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "tk=Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "X_train_tk=tk.texts_to_sequences(X_train)\n",
    "X_test_tk=tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tk, dtype='float32', padding='post', maxlen=150)\n",
    "X_test_pad = pad_sequences(X_test_tk, dtype='float32', padding='post', maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 1D Convolution.\n",
    "\n",
    "❓ **Question** ❓ Define a model that has :\n",
    "- an `Embedding` layer: `input_dim` is the `vocab_size + 1`, `output_dim` is the embedding space dimension, and `mask_zero` has to be set to `True`. Here, for computational reasons, set `input_length` to the maximum length of your observations (that you just defined in the previous question).\n",
    "- a `Conv1D` layer \n",
    "- a `Flatten` layer\n",
    "- a `Dense` layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly\n",
    "\n",
    "❗ **Remark** ❗ The size of the `Conv1D` kernel corresponds exactly to the number of side-by-side words (tokens) each kernel is taking into account ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:47.845727Z",
     "start_time": "2023-02-24T13:52:47.810352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:47.878706Z",
     "start_time": "2023-02-24T13:52:47.847857Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 70\n",
    "\n",
    "def my_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=vocab_size+1, #+1 for the 0 padding\n",
    "        input_length=150,\n",
    "        output_dim=embedding_size, \n",
    "        mask_zero=True, # Built-in masking layer :)\n",
    "    ))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv1D(20,kernel_size=150, activation='tanh'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:47.913595Z",
     "start_time": "2023-02-24T13:52:47.880573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42660"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the number of parameters. You can compare it to the model that you had in previous exercise (esp. the first one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:48.049428Z",
     "start_time": "2023-02-24T13:52:47.915551Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 70)           2986270   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1, 20)             210020    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,196,511\n",
      "Trainable params: 3,196,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = my_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model with a stopping criterion, and evaluate it on the test data.\n",
    "\n",
    "You will probably notice that it is ... **much faster** than RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:52:59.908848Z",
     "start_time": "2023-02-24T13:52:48.051394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.6762 - accuracy: 0.5746 - val_loss: 0.6101 - val_accuracy: 0.7047\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.2885 - accuracy: 0.9254 - val_loss: 0.4842 - val_accuracy: 0.7620\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0340 - accuracy: 0.9969 - val_loss: 0.5402 - val_accuracy: 0.7660\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 0.7773\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 3.0245e-04 - accuracy: 1.0000 - val_loss: 0.9879 - val_accuracy: 0.7633\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 2.4014e-05 - accuracy: 1.0000 - val_loss: 1.5234 - val_accuracy: 0.7407\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 1.9322e-06 - accuracy: 1.0000 - val_loss: 2.0260 - val_accuracy: 0.7233\n",
      "The accuracy evaluated on the test set is of 77.780%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model = my_model()\n",
    "model.fit(X_train_pad, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Learn a Word2Vec representation, and then feed it into a NN with a `Conv1D`🔥 \n",
    "\n",
    "In the first part of the exercise, you were asked to jointly learn the embedding representation and the CNN convolution within the same network, which was the CNN counterpart of the left part of this Figure: \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "Now, let's try to replace the RNN with a CNN for an architecture as on the right side.\n",
    "\n",
    "❓ **Question** ❓ Learn a word2vec model or load a trained one directly from GENSIM (transfer learning). Then, prepare your data as in the previous exercise. This question is quite long, it prepares you for real world challenges - but you have already done all the building bricks in the previous exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:53:01.864744Z",
     "start_time": "2023-02-24T13:52:59.912231Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=X_train, vector_size=130, min_count=7, window=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:53:05.580875Z",
     "start_time": "2023-02-24T13:53:01.867018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad2 = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad2 = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now construct a model that has a `Conv1D` layer, a flatten layer, a dense layer, and an output layer. Compile it, and fit it on the train data. You can then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:53:50.748095Z",
     "start_time": "2023-02-24T13:53:50.713038Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def my_model2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Masking(mask_value=0))\n",
    "\n",
    "    model.add(layers.Conv1D(20,kernel_size=150, activation='tanh'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T13:54:27.539456Z",
     "start_time": "2023-02-24T13:53:50.933381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 5s 46ms/step - loss: 0.7018 - accuracy: 0.5334 - val_loss: 0.6976 - val_accuracy: 0.6100\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 0.6306 - accuracy: 0.6497 - val_loss: 0.6447 - val_accuracy: 0.6307\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 5s 44ms/step - loss: 0.4919 - accuracy: 0.7411 - val_loss: 0.7019 - val_accuracy: 0.6153\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 5s 44ms/step - loss: 0.3087 - accuracy: 0.8731 - val_loss: 0.8198 - val_accuracy: 0.6047\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 5s 47ms/step - loss: 0.1643 - accuracy: 0.9454 - val_loss: 0.9540 - val_accuracy: 0.6080\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 5s 43ms/step - loss: 0.0840 - accuracy: 0.9737 - val_loss: 1.3346 - val_accuracy: 0.5633\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 5s 43ms/step - loss: 0.0462 - accuracy: 0.9874 - val_loss: 1.3193 - val_accuracy: 0.5987\n",
      "The accuracy evaluated on the test set is of 61.620%\n"
     ]
    }
   ],
   "source": [
    "model2 = my_model2()\n",
    "model2.fit(X_train_pad2, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "\n",
    "res2 = model2.evaluate(X_test_pad2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res2[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ You might be frustrated by the accuracy you got, this happens to us all at some point. Once you have tested your first iteration, you need to improve your models: by making them more complex, changing the parameters, stacking additional layers, and so on.\n",
    "\n",
    "Only practice and experimentation will get you there. So you can go back to your previous models, change them and try to get better results ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T15:46:17.997301Z",
     "start_time": "2023-02-24T15:46:13.525874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master a07eba2] Completed Challenge NPL with CNN\n",
      " 1 file changed, 408 insertions(+), 22 deletions(-)\n",
      "Enumerating objects: 10, done.\n",
      "Counting objects: 100% (10/10), done.\n",
      "Delta compression using up to 16 threads\n",
      "Compressing objects: 100% (10/10), done.\n",
      "Writing objects: 100% (10/10), 741.43 KiB | 5.53 MiB/s, done.\n",
      "Total 10 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), done.\u001b[K\n",
      "To github.com:CedrikGiau/data-nlp-with-cnn.git\n",
      " * [new branch]      master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add NLP-with-CNN.ipynb\n",
    "\n",
    "!git commit -m 'Completed Challenge NPL with CNN'\n",
    "\n",
    "!git push origin master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
